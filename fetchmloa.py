# -*- coding: utf-8 -*-
"""FetchMLoa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fY4ZMSYeTIDPCMw20c-VizSmKmm0ZNzi
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as rcParams
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf,plot_pacf



def plot_data(data):
  plt.figure(figsize=(25,10))
  plt.plot()

"""### Augmented Dickey-Fuller Test(ADF Test): Used to test the stationary of a time series.
The null hypothesis is that the series is stationary. It this series reject the null hypothesis, then I have to convert the series to stationary. 
"""

def adf_check(time_series):
    """
    Pass in a time series, returns ADF report
    """
    result = adfuller(time_series)
    mes = 'Augmented Dickey-Fuller Test: \n'
    labels = ['ADF Test Statistic','p-value','Number of Lags Used','Number of Observations Used']

    for value,label in zip(result,labels):
        mes += label + ' : '+str(value) + '\n'
    
    if result[1] <= 0.05:
        mes += "strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary"
    else:
        mes += "weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \n"
    return mes
        
def data_t(data):
  data_testing = pd.DataFrame(np.log(data['Receipt_Count']).diff().diff(28))
  message = adf_check(data_testing['Receipt_Count'].dropna())
  return message


"""### Check Auto-Correlated function(ACF) and partial ACF(PACF).
ACF used to describe the relation between the current term and all the lagged terms, while PACF only focus on the relation between the current term and the selected lagged term.
"""
def ACF_plot(data):
  data_testing = pd.DataFrame(np.log(data['Receipt_Count']).diff().diff(28))
  ACF = plot_acf(data_testing.dropna(),lags=30)
  PACF = plot_pacf(data_testing.dropna(),lags=30)
  return ACF, PACF

"""We can see that both plots decline gradually. Thus, we can use ARMA model.
In the PACF plot, the value converge to 0 at 8, so we can set the number of AR term, "p", to 8. In the ACF plot, the value becomes unsignificant at 2, so we can set the parameter for MA ,"q", to be 2.

### Aotu Regressive
Use linear regression to calculate how does the value of the current term depends on its p previous lagged terms. The model uses coefficients in linear regression for each lagged term to represent the relation.
"""

def AR(p,data):
  data_temp = data

  #Generating the lagged p terms
  for i in range(1,p+1):
    data_temp['Shifted_values_%d' % i ] = data_temp['Receipt_Count'].shift(i)

  train_size = (int)(0.8 * data_temp.shape[0])

  #Breaking data set into test and training
  data_train = pd.DataFrame(data_temp[0:train_size])
  data_test = pd.DataFrame(data_temp[train_size:data.shape[0]])

  data_train_2 = data_train.dropna()
  #X contains the lagged values ,hence we skip the first column
  X_train = data_train_2.iloc[:,1:].values.reshape(-1,p)
  #Y contains the value,it is the first column
  y_train = data_train_2.iloc[:,0].values.reshape(-1,1)

  #Running linear regression to generate the coefficents of lagged terms
  from sklearn.linear_model import LinearRegression
  lr = LinearRegression()
  lr.fit(X_train,y_train)

  theta  = lr.coef_.T
  intercept = lr.intercept_
  data_train_2['Predicted_Values'] = X_train.dot(lr.coef_.T) + lr.intercept_

  X_test = data_test.iloc[:,1:].values.reshape(-1,p)
  data_test['Predicted_Values'] = X_test.dot(lr.coef_.T) + lr.intercept_

  RMSE = np.sqrt(mean_squared_error(data_test['Receipt_Count'], data_test['Predicted_Values']))

  mes = "The RMSE is :" + str(RMSE) + ", Value of p : " + str(p)
  return [data_train_2,data_test,theta,intercept,RMSE, mes]

# [data_train,data_test,theta,intercept,RMSE] = AR(8,pd.DataFrame(data_testing['Receipt_Count']))

"""By the PACF plot, I set the p equal 8. The RMSE is 0.0304 so it's small enough. \\
Use the plot to check the fitness.
"""

# data_c = pd.concat([data_train,data_test])
# data_c[['Receipt_Count','Predicted_Values']].iloc[0:50].plot(figsize=(10,10))

"""Generating the residuals for MA model. Also check the residual plot to see if there are skewness. """

# res = pd.DataFrame()
# res['Residuals'] = data_c['Receipt_Count'] - data_c.Predicted_Values

# res.plot(kind='kde')

"""### Moving Average
Use linear regression to solve the number of lagged forecast errors in the prediction equaion. Also this step can help remove noise and smooth data. 
"""

def MA(q,res):

  for i in range(1,q+1):
    res['Shifted_values_%d' % i ] = res['Residuals'].shift(i)

  train_size = (int)(0.8 * res.shape[0])

  res_train = pd.DataFrame(res[0:train_size])
  res_test = pd.DataFrame(res[train_size:res.shape[0]])

  res_train_2 = res_train.dropna()
  X_train = res_train_2.iloc[:,1:].values.reshape(-1,q)
  y_train = res_train_2.iloc[:,0].values.reshape(-1,1)

  from sklearn.linear_model import LinearRegression
  lr = LinearRegression()
  lr.fit(X_train,y_train)

  theta  = lr.coef_.T
  intercept = lr.intercept_
  res_train_2['Predicted_Values'] = X_train.dot(lr.coef_.T) + lr.intercept_
  # res_train_2[['Residuals','Predicted_Values']].plot()

  X_test = res_test.iloc[:,1:].values.reshape(-1,q)
  res_test['Predicted_Values'] = X_test.dot(lr.coef_.T) + lr.intercept_
  res_test[['Residuals','Predicted_Values']].plot()

  from sklearn.metrics import mean_squared_error
  RMSE = np.sqrt(mean_squared_error(res_test['Residuals'], res_test['Predicted_Values']))

  mes = "The RMSE is :" + str(RMSE) + ", Value of q : " + str(q)
  return [res_train_2,res_test,theta,intercept,RMSE, mes]

# [res_train,res_test,theta_res,intercept_res,RMSE] = MA(2,pd.DataFrame(res.Residuals))

"""Merge the residual prediction back to the prediction value. We can see that now the predicted data fits better with the original data. """

# res_c = pd.concat([res_train,res_test])
# data_c.Predicted_Values += res_c.Predicted_Values

# data_c[['Receipt_Count','Predicted_Values']].iloc[0:50].plot(figsize=(10,10))

"""### Getting back Origin Data
Reversing the steps performed for differencing. Check the first column is the same after reversing the steps.
"""

# data_c.Receipt_Count += pd.DataFrame(np.log(data['Receipt_Count'])).shift(1).Receipt_Count
# data_c.Receipt_Count += pd.DataFrame(np.log(data['Receipt_Count'])).diff().shift(28).Receipt_Count
# data_c.Predicted_Values += pd.DataFrame(np.log(data['Receipt_Count'])).shift(1).Receipt_Count 
# data_c.Predicted_Values += pd.DataFrame(np.log(data['Receipt_Count'])).diff().shift(28).Receipt_Count
# data_c.Receipt_Count = np.exp(data_c['Receipt_Count'])
# data_c.Predicted_Values = np.exp(data_c.Predicted_Values)


# data_c[['Receipt_Count','Predicted_Values']].plot(figsize=(25,10))

"""As you can see from the above plot, the predicted value generally follow the origin data, and this model is valid for further prediction. \\"""

def Pred(theta, inter, res_theta, res_inter):
  return